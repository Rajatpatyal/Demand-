import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Concatenate, Input
from tensorflow.keras.models import Model
from sklearn.preprocessing import OneHotEncoder

# Load and preprocess your dataset
# The dataset should contain columns like 'date', 'demand', 'holidays', 'festivals', 'weekly_offs', 'covid_cases'
# Perform more advanced feature engineering as needed

# Load and preprocess the dataset
# df = pd.read_csv('your_dataset.csv')
# Perform data preprocessing, handle missing values, and create relevant features

# Feature Engineering
# For holidays, festivals, and weekly offs, convert them into one-hot encoded features
encoder = OneHotEncoder(sparse=False)
holiday_features = encoder.fit_transform(df[['holidays']])
festival_features = encoder.fit_transform(df[['festivals']])
weekly_off_features = encoder.fit_transform(df[['weekly_offs']])

# Combine all features
additional_features = np.concatenate((holiday_features, festival_features, weekly_off_features), axis=1)

# Combine with other features
processed_features = np.concatenate((df[['covid_cases']], additional_features), axis=1)

# Split data into training and testing sets
train_size = int(0.8 * len(df))
train_data = processed_features[:train_size]
test_data = processed_features[train_size:]
train_demand = df['demand'][:train_size]
test_demand = df['demand'][train_size:]

# Normalize the demand data
scaler = MinMaxScaler()
train_demand = scaler.fit_transform(train_demand.values.reshape(-1, 1))
test_demand = scaler.transform(test_demand.values.reshape(-1, 1))

# Create sequences for input data
def create_sequences(data, seq_length):
    sequences = []
    targets = []
    for i in range(len(data) - seq_length):
        seq = data[i:i+seq_length]
        target = data[i+seq_length:i+seq_length+1]
        sequences.append(seq)
        targets.append(target)
    return np.array(sequences), np.array(targets)

seq_length = 12  # Adjust as needed
train_sequences, train_targets = create_sequences(train_data, seq_length)
test_sequences, test_targets = create_sequences(test_data, seq_length)

# Build the model
model = Sequential([
    LSTM(128, activation='relu', input_shape=(seq_length, train_data.shape[2]), return_sequences=True),
    Dropout(0.2),
    LSTM(64, activation='relu', return_sequences=False),
    Dropout(0.2),
    Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(train_sequences, train_targets, epochs=50, batch_size=32, validation_data=(test_sequences, test_targets))

# Forecasting
last_sequence = train_sequences[-1:]
forecasted_demand = []

for _ in range(12):  # Forecast for 12 months
    forecast = model.predict(last_sequence)
    forecasted_demand.append(forecast[0, 0])
    last_sequence = np.roll(last_sequence, -1, axis=1)
    last_sequence[:, -1] = forecast

# Denormalize the forecasted demand
forecasted_demand = scaler.inverse_transform(np.array(forecasted_demand).reshape(-1, 1))

print("Forecasted Demand for the next 12 months:", forecasted_demand)
